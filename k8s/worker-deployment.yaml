# # k8s/worker-deployment.yaml
# apiVersion: apps/v1
# kind: Deployment
# metadata:
#   name: dask-worker
# spec:
#   replicas: 2
#   selector:
#     matchLabels:
#       app: dask-worker
#   template:
#     metadata:
#       labels:
#         app: dask-worker
#     spec:
#       volumes:
#         - name: data
#           hostPath:
#             path: /data
#             type: Directory
#         - name: output
#           hostPath:
#             path: /output
#             type: Directory
#       containers:
#       - name: worker
#         image: dasymetric:latest
#         imagePullPolicy: Never

#         command:
#           - dask
#           - worker
#           - tcp://dask-scheduler:8786
#           - "--nworkers"
#           - "2"
#           - "--nthreads"
#           - "6"
#           - "--memory-limit"
#           - "3GB"
#         env:
#           - name: DASK_SCHEDULER
#             value: tcp://dask-scheduler:8786
#         volumeMounts:
#           - name: data
#             mountPath: /data
#           - name: output
#             mountPath: /output
# k8s/worker-deployment-aks.yaml
# apiVersion: apps/v1
# kind: Deployment
# metadata:
#   name: dask-worker
# spec:
#   replicas: 10
#   selector:
#     matchLabels: { app: dask-worker }
#   template:
#     metadata:
#       labels: { app: dask-worker }
#     spec:
#       # nodeSelector:
#       #     agentpool: test 
#       imagePullSecrets:
#         - name: acr-auth
#       volumes:
#         - name: data
#           persistentVolumeClaim:
#             claimName: data-pvc
#         - name: output
#           persistentVolumeClaim:
#             claimName: output-pvc
#       containers:
#       - name: worker
#         image: popaloc.azurecr.io/samples/dasymetric:latest
#         command:
#           - dask
#           - worker
#           - tcp://dask-scheduler:8786
#           - "--nthreads"
#           - "2"
#         resources:
#           requests:
#             cpu: "2"
#             memory: "12Gi"
#           limits:
#             cpu: "3"
#             memory: "16Gi"

#         volumeMounts:
#           - name: data
#             mountPath: /data
#           - name: output
#             mountPath: /output
      
      

apiVersion: apps/v1
kind: Deployment
metadata:
  name: dask-worker
spec:
  replicas: 48
  selector:
    matchLabels: { app: dask-worker }
  template:
    metadata:
      labels: { app: dask-worker }
    spec:
      # Pin to your user pool (uncomment + set):
      # nodeSelector:
      #   kubernetes.azure.com/agentpool: <your-worker-pool>
      # tolerations:
      # - key: "raster"         # if you tainted the pool
      #   operator: "Equal"
      #   value: "true"
      #   effect: "NoSchedule"

      terminationGracePeriodSeconds: 60
      imagePullSecrets:
        - name: acr-auth

      # Spread workers evenly across nodes
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector: { matchLabels: { app: dask-worker } }

      volumes:
        - name: data
          persistentVolumeClaim: { claimName: data-pvc }
        - name: output
          persistentVolumeClaim: { claimName: output-pvc }
        - name: dask-tmp                   # NVMe-backed spill/scratch
          emptyDir:
            sizeLimit: "100Gi"

      containers:
      - name: worker
        image: popaloc.azurecr.io/samples/dasymetric:latest
        command:
          - dask
          - worker
          - tcp://dask-scheduler:8786
          - "--nthreads"                   # 1 thread â†’ no GIL contention
          - "1"
          - "--memory-limit"               # align with requests
          - "12Gi"
        env:
          - name: DASK_TEMPORARY_DIRECTORY # spill to local NVMe
            value: /dask-tmp
          - name: DASK_DISTRIBUTED__SCHEDULER__WORK_STEALING
            value: "True"
          # Optional: tune memory spill thresholds (helps avoid OOM/slowdowns)
          - name: DASK_DISTRIBUTED__WORKER__MEMORY__TARGET
            value: "0.7"
          - name: DASK_DISTRIBUTED__WORKER__MEMORY__SPILL
            value: "0.9"
          - name: DASK_DISTRIBUTED__WORKER__MEMORY__PAUSE
            value: "0.8"
          - name: DASK_DISTRIBUTED__WORKER__MEMORY__TERMINATE
            value: "0.95"
        resources:
          requests:
            cpu: "2"
            memory: "8Gi"
          limits:
            cpu: "3"
            memory: "16Gi"
        volumeMounts:
          - name: data
            mountPath: /data
            readOnly: true            # safer/faster for shared inputs
          - name: output
            mountPath: /output
          - name: dask-tmp
            mountPath: /dask-tmp

























