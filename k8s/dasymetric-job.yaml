# # k8s/dasymetric-job.yaml
# apiVersion: batch/v1
# kind: Job
# metadata:
#   name: dasymetric-run
# spec:
#   template:
#     spec:
#       restartPolicy: Never
#       volumes:
#         - name: data
#           hostPath:
#             path: /data    # see step 4
#             type: Directory
#         - name: output
#           hostPath:
#             path: /output # see step 4
#             type: Directory
#       containers:
#       - name: dasymetric
#         image: dasymetric:latest
#         imagePullPolicy: Never
#         command: ["python", "dasymetric_dask.py"]
#         env:
#           - name: DASK_SCHEDULER
#             value: tcp://dask-scheduler:8786
#           - name: CENSUS_FILE
#             value: /data/galveston_blocks_Projection.shp
#           - name: LULC_FILE
#             value: /data/Annual_NLCD_LndCov_2023_C.tif
#           - name: OUTPUT_DIR
#             value: /output
#           - name: TILE_SIZE
#             value: "1000"
#           - name: POP_FIELD
#             value: POP20
#           - name: BATCH_SIZE
#             value: "100"
#           - name: PYTHONUNBUFFERED
#             value: "1"
#         volumeMounts:
#           - name: data
#             mountPath: /data
#           - name: output
#             mountPath: /output



# # k8s/dasymetric-job-aks.yaml
# apiVersion: batch/v1
# kind: Job
# metadata:
#   name: dasymetric-run
# spec:
#   ttlSecondsAfterFinished: 60
#   template:
#     spec:
#       restartPolicy: Never
#       # imagePullSecrets:
#       #   - name: acr-auth
#       volumes:
#         - name: data
#           persistentVolumeClaim:
#             claimName: data-pvc
#         - name: output
#           persistentVolumeClaim:
#             claimName: output-pvc
#       containers:
#         - name: dasymetric
#           image: popaloc.azurecr.io/samples/dasymetric:latest
#           command: ["python", "dasymetric_dask.py"]
#           env:
#             - name: DASK_SCHEDULER
#               value: tcp://dask-scheduler:8786
#             - name: CENSUS_FILE
#               value: /data/tl_2024_48_tabblock20.shp
#               #value: /data/galveston_blocks_Projectio.shp
#             - name: LULC_FILE
#               value: /data/NLCD_Texas.tif
#               #value: /data/Annual_NLCD_LndCov_2023_C.tif
#             - name: OUTPUT_DIR
#               value: /output
#             - name: TILE_SIZE
#               value: "1000"
#             - name: POP_FIELD
#               value: POP20
#             - name: BATCH_SIZE
#               value: "100"
#           volumeMounts:
#            - name: data
#              mountPath: /data
#            - name: output
#              mountPath: /output

apiVersion: batch/v1
kind: Job
metadata:
  name: dasymetric-run
spec:
  backoffLimit: 0
  ttlSecondsAfterFinished: 60
  template:
    spec:
      restartPolicy: Never
      nodeSelector:
            agentpool: test
      terminationGracePeriodSeconds: 60
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: data-pvc
        - name: output
          persistentVolumeClaim:
            claimName: output-pvc
        - name: dask-tmp
          emptyDir:
            sizeLimit: "100Gi"     # spill/scratch; adjust as needed
      containers:
        - name: dasymetric
          image: popaloc.azurecr.io/samples/dasymetric:latest
          command: ["python", "dasymetric_dask.py"]
          resources:
            requests:
              cpu: "1"
              memory: "4Gi"
            limits:
              cpu: "2"
              memory: "6Gi"      # raise if you still OOM
          env:
            - name: DASK_SCHEDULER
              value: tcp://dask-scheduler:8786
            - name: CENSUS_FILE
              #value: /data/galveston_blocks_Projection.shp
              value: /data/tl_2024_48_tabblock20.shp
            - name: LULC_FILE
              value: /data/NLCD_Texas.tif
              #value: /data/Annual_NLCD_LndCov_2023_C.tif
            - name: OUTPUT_DIR
              value: /output
            - name: TILE_SIZE
              value: "512"        # ↓ from 1000 to cut memory per task
            - name: POP_FIELD
              value: POP20
            - name: BATCH_SIZE
              value: "20"         # ↓ from 100 to reduce working set
            # Raster/GDAL cache: keep it modest so it doesn't eat RAM
            - name: GDAL_CACHEMAX
              value: "512"        # MB
            # Dask spill-to-disk thresholds (client may inherit, but workers need their own config—see §3)
            - name: DASK_TEMPORARY_DIRECTORY
              value: /dask-tmp
            - name: DASK_DISTRIBUTED__WORKER__MEMORY__TARGET
              value: "0.7"
            - name: DASK_DISTRIBUTED__WORKER__MEMORY__SPILL
              value: "0.9"
            - name: DASK_DISTRIBUTED__WORKER__MEMORY__PAUSE
              value: "0.8"
            - name: DASK_DISTRIBUTED__WORKER__MEMORY__TERMINATE
              value: "0.95"
            - name: PYTHONUNBUFFERED
              value: "1"
            - name: PYTHONFAULTHANDLER
              value: "1"
          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - name: data
              mountPath: /data
            - name: output
              mountPath: /output
            - name: dask-tmp
              mountPath: /dask-tmp
          



















