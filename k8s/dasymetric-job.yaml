# # k8s/dasymetric-job.yaml
# apiVersion: batch/v1
# kind: Job
# metadata:
#   name: dasymetric-run
# spec:
#   template:
#     spec:
#       restartPolicy: Never
#       volumes:
#         - name: data
#           hostPath:
#             path: /data    # see step 4
#             type: Directory
#         - name: output
#           hostPath:
#             path: /output # see step 4
#             type: Directory
#       containers:
#       - name: dasymetric
#         image: dasymetric:latest
#         imagePullPolicy: Never
#         command: ["python", "dasymetric_dask.py"]
#         env:
#           - name: DASK_SCHEDULER
#             value: tcp://dask-scheduler:8786
#           - name: CENSUS_FILE
#             value: /data/galveston_blocks_Projection.shp
#           - name: LULC_FILE
#             value: /data/Annual_NLCD_LndCov_2023_C.tif
#           - name: OUTPUT_DIR
#             value: /output
#           - name: TILE_SIZE
#             value: "1000"
#           - name: POP_FIELD
#             value: POP20
#           - name: BATCH_SIZE
#             value: "100"
#           - name: PYTHONUNBUFFERED
#             value: "1"
#         volumeMounts:
#           - name: data
#             mountPath: /data
#           - name: output
#             mountPath: /output



# # k8s/dasymetric-job-aks.yaml
# apiVersion: batch/v1
# kind: Job
# metadata:
#   name: dasymetric-run
# spec:
#   ttlSecondsAfterFinished: 60
#   template:
#     spec:
#       restartPolicy: Never
#       # imagePullSecrets:
#       #   - name: acr-auth
#       volumes:
#         - name: data
#           persistentVolumeClaim:
#             claimName: data-pvc
#         - name: output
#           persistentVolumeClaim:
#             claimName: output-pvc
#       containers:
#         - name: dasymetric
#           image: popaloc.azurecr.io/samples/dasymetric:latest
#           command: ["python", "dasymetric_dask.py"]
#           env:
#             - name: DASK_SCHEDULER
#               value: tcp://dask-scheduler:8786
#             - name: CENSUS_FILE
#               value: /data/tl_2024_48_tabblock20.shp

#               #value: /data/galveston_blocks_Projectio.shp
#             - name: LULC_FILE
#               value: /data/NLCD_Texas.tif
#               #value: /data/Annual_NLCD_LndCov_2023_C.tif
#             - name: OUTPUT_DIR
#               value: /output
#             - name: TILE_SIZE
#               value: "1000"
#             - name: POP_FIELD
#               value: POP20
#             - name: BATCH_SIZE
#               value: "100"
#           volumeMounts:
#            - name: data
#              mountPath: /data
#            - name: output
#              mountPath: /output

# apiVersion: batch/v1
# kind: Job
# metadata:
#   name: dasymetric-run
# spec:
#   backoffLimit: 0
#   ttlSecondsAfterFinished: 60
#   template:
#     spec:
#       restartPolicy: Never
#       # nodeSelector:
#       #       kubernetes.azure.com/agentpool: test
#       terminationGracePeriodSeconds: 60
#       volumes:
#         - name: data
#           persistentVolumeClaim:
#             claimName: data-pvc
#         - name: output
#           persistentVolumeClaim:
#             claimName: output-pvc
#         - name: dask-tmp
#           emptyDir:
#             sizeLimit: "100Gi"     # spill/scratch; adjust as needed
#       containers:
#         - name: dasymetric
#           image: popaloc.azurecr.io/samples/dasymetric:latest
#           command: ["python", "dasymetric_dask.py"]
#           resources:
#             requests:
#               cpu: "4"
#               memory: "6Gi"
#             limits:
#               cpu: "6"
#               memory: "8Gi"      # raise if you still OOM
#           env:
#             - name: DASK_SCHEDULER
#               value: tcp://dask-scheduler:8786
#             - name: CENSUS_FILE
#               #value: /data/galveston_blocks_Projection.shp
#               value: /data/tl_2024_48_tabblock20.shp
#             - name: LULC_FILE
#               value: /data/NLCD_Texas.tif
#               #value: /data/Annual_NLCD_LndCov_2023_C.tif
#             - name: OUTPUT_DIR
#               value: /output
#             - name: TILE_SIZE
#               value: "1028"        # ↓ from 1000 to cut memory per task
#             - name: POP_FIELD
#               value: POP20
#             - name: BATCH_SIZE
#               value: "20"         # ↓ from 100 to reduce working set
#             # Raster/GDAL cache: keep it modest so it doesn't eat RAM
#             - name: GDAL_CACHEMAX
#               value: "512"        # MB
#             # Dask spill-to-disk thresholds (client may inherit, but workers need their own config—see §3)
#             - name: DASK_TEMPORARY_DIRECTORY
#               value: /dask-tmp
#             - name: DASK_DISTRIBUTED__WORKER__MEMORY__TARGET
#               value: "0.7"
#             - name: DASK_DISTRIBUTED__WORKER__MEMORY__SPILL
#               value: "0.9"
#             - name: DASK_DISTRIBUTED__WORKER__MEMORY__PAUSE
#               value: "0.8"
#             - name: DASK_DISTRIBUTED__WORKER__MEMORY__TERMINATE
#               value: "0.95"
#             - name: PYTHONUNBUFFERED
#               value: "1"
#             - name: PYTHONFAULTHANDLER
#               value: "1"
#           terminationMessagePolicy: FallbackToLogsOnError
#           volumeMounts:
#             - name: data
#               mountPath: /data
#             - name: output
#               mountPath: /output
#             - name: dask-tmp
#               mountPath: /dask-tmp


apiVersion: batch/v1
kind: Job
metadata:
  name: dasymetric-run
  labels:
    app: dasymetric
spec:
  completions: 1              # <-- run exactly one successful completion
  parallelism: 1              # <-- don't create multiple concurrent client pods
  backoffLimit: 0
  ttlSecondsAfterFinished: 60
  template:
    metadata:
      labels:
        app: dasymetric
    spec:
      restartPolicy: Never
      terminationGracePeriodSeconds: 60

      # --- Pin this job to your new user pool (optional but recommended) ---
      # 1) Replace <your-user-pool-name> with the AKS agent pool name.
      # 2) If you tainted the pool (e.g., raster=true:NoSchedule), also
      #    uncomment the tolerations block below.
      # nodeSelector:
      #   kubernetes.azure.com/agentpool: <your-user-pool-name>
      # tolerations:
      # - key: "raster"
      #   operator: "Equal"
      #   value: "true"
      #   effect: "NoSchedule"

      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: data-pvc
        - name: output
          persistentVolumeClaim:
            claimName: output-pvc
        - name: dask-tmp
          emptyDir:
            sizeLimit: "100Gi"   # spill/scratch for Dask & intermediates

      containers:
        - name: dasymetric
          image: popaloc.azurecr.io/samples/dasymetric:latest
          command: ["python", "dasymetric_dask.py"]
          # This pod is the Dask *client* that orchestrates work; keep sizing modest but safe.
          resources:
            requests:
              cpu: "4"
              memory: "20Gi"
            limits:
              cpu: "6"
              memory: "21Gi"
          env:
            - name: DASK_SCHEDULER
              value: tcp://dask-scheduler:8786

            # --- Input data paths (from PVCs) ---
            - name: CENSUS_FILE
              value: /data/texas_parquet.parquet
              #value: /data/east_texas_parquet.parquet
              #value: /data/harris.shp
            - name: LULC_FILE
              value: /data/NLCD_Texas.tif
              #value: /data/NLCD_Harris.tif
              #value: /data/NLCD_east_texas.tif

            # --- Output directory (PVC) ---
            - name: OUTPUT_DIR
              value: /output

            # --- Work partitioning knobs (tune based on worker count/memory) ---
            - name: TILE_SIZE
              value: "512"       # was ~1000; adjust to balance per-task RAM
            - name: POP_FIELD
              value: POP20
            - name: BATCH_SIZE
              value: "20"         # lower batch reduces in-memory working set

            # --- GDAL/RasterIO cache sizing (in MB) ---
            - name: GDAL_CACHEMAX
              value: "512"

            # --- Dask spill/pressure thresholds for safety under load ---
            - name: DASK_TEMPORARY_DIRECTORY
              value: /dask-tmp
            - name: DASK_DISTRIBUTED__WORKER__MEMORY__TARGET
              value: "0.7"
            - name: DASK_DISTRIBUTED__WORKER__MEMORY__SPILL
              value: "0.9"
            - name: DASK_DISTRIBUTED__WORKER__MEMORY__PAUSE
              value: "0.8"
            - name: DASK_DISTRIBUTED__WORKER__MEMORY__TERMINATE
              value: "0.95"

            # --- Quality-of-life/debug ---
            - name: PYTHONUNBUFFERED
              value: "1"
            - name: PYTHONFAULTHANDLER
              value: "1"

          terminationMessagePolicy: FallbackToLogsOnError
          volumeMounts:
            - name: data
              mountPath: /data
            - name: output
              mountPath: /output
            - name: dask-tmp
              mountPath: /dask-tmp

          











































